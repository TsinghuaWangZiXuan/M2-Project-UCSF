{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2e452-e86f-4fda-85a5-01aa389eacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rastermap import Rastermap, utils\n",
    "from scipy.stats import zscore\n",
    "import scipy\n",
    "import seaborn\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn import decomposition, manifold\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051d529-9447-4c51-b1c5-8fb94dd55ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from scipy.sparse import csc_matrix\n",
    "def get_contours(A, dims, thr=0.9, thr_method='nrg', swap_dim=False, slice_dim = None):\n",
    "    \"\"\"Gets contour of spatial components and returns their coordinates\n",
    "\n",
    "     Args:\n",
    "         A:   np.ndarray or sparse matrix\n",
    "                   Matrix of Spatial components (d x K)\n",
    "\n",
    "             dims: tuple of ints\n",
    "                   Spatial dimensions of movie\n",
    "\n",
    "             thr: scalar between 0 and 1\n",
    "                   Energy threshold for computing contours (default 0.9)\n",
    "\n",
    "             thr_method: string\n",
    "                  Method of thresholding:\n",
    "                      'max' sets to zero pixels that have value less than a fraction of the max value\n",
    "                      'nrg' keeps the pixels that contribute up to a specified fraction of the energy\n",
    "            \n",
    "             swap_dim: bool\n",
    "                  If False (default), each column of A should be reshaped in F-order to recover the mask;\n",
    "                  this is correct if the dimensions have not been reordered from (y, x[, z]).\n",
    "                  If True, each column should be reshaped in C-order; this is correct for dims = ([z, ]x, y).\n",
    "\n",
    "             slice_dim: int or None\n",
    "                  Which dimension to slice along if we have 3D data. (i.e., get contours on each plane along this axis).\n",
    "                  The default (None) is 0 if swap_dim is True, else -1.\n",
    "\n",
    "     Returns:\n",
    "         Coor: list of coordinates with center of mass and\n",
    "                contour plot coordinates (per layer) for each component\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    A = csc_matrix((A['data'][:], A['indices'][:], A['indptr'][:]), shape=A['shape'][:])\n",
    "    d, nr = np.shape(A)\n",
    "\n",
    "    coordinates = []\n",
    "\n",
    "    # for each patches\n",
    "    for i in range(nr):\n",
    "        pars:dict = dict()\n",
    "        # we compute the cumulative sum of the energy of the Ath component that has been ordered from least to highest\n",
    "        patch_data = A.data[A.indptr[i]:A.indptr[i + 1]]\n",
    "        indx = np.argsort(patch_data)[::-1]\n",
    "        if thr_method == 'nrg':\n",
    "            cumEn = np.cumsum(patch_data[indx]**2)\n",
    "            if len(cumEn) == 0:\n",
    "                pars = dict(\n",
    "                    coordinates=np.array([]),\n",
    "                    CoM=np.array([np.NaN, np.NaN]),\n",
    "                    neuron_id=i + 1,\n",
    "                )\n",
    "                coordinates.append(pars)\n",
    "                continue\n",
    "            else:\n",
    "                # we work with normalized values\n",
    "                cumEn /= cumEn[-1]\n",
    "                Bvec = np.ones(d)\n",
    "                # we put it in a similar matrix\n",
    "                Bvec[A.indices[A.indptr[i]:A.indptr[i + 1]][indx]] = cumEn\n",
    "        else:\n",
    "            if thr_method != 'max':\n",
    "                warn(\"Unknown threshold method. Choosing max\")\n",
    "            Bvec = np.zeros(d)\n",
    "            Bvec[A.indices[A.indptr[i]:A.indptr[i + 1]]] = patch_data / patch_data.max()\n",
    "\n",
    "        if swap_dim:\n",
    "            Bmat = np.reshape(Bvec, dims, order='C')\n",
    "        else:\n",
    "            Bmat = np.reshape(Bvec, dims, order='F')\n",
    "\n",
    "        def get_slice_coords(B: np.ndarray) -> np.ndarray:\n",
    "            \"\"\"Get contour coordinates for a 2D slice\"\"\"\n",
    "            d1, d2 = B.shape\n",
    "            vertices = find_contours(B.T, thr)\n",
    "            # this fix is necessary for having disjoint figures and borders plotted correctly\n",
    "            v = np.atleast_2d([np.nan, np.nan])\n",
    "            for _, vtx in enumerate(vertices):\n",
    "                num_close_coords = np.sum(np.isclose(vtx[0, :], vtx[-1, :]))\n",
    "                if num_close_coords < 2:\n",
    "                    if num_close_coords == 0:\n",
    "                        # case angle\n",
    "                        newpt = np.round(np.mean(vtx[[0, -1], :], axis=0) / [d2, d1]) * [d2, d1]\n",
    "                        vtx = np.concatenate((newpt[np.newaxis, :], vtx, newpt[np.newaxis, :]), axis=0)\n",
    "                    else:\n",
    "                        # case one is border\n",
    "                        vtx = np.concatenate((vtx, vtx[0, np.newaxis]), axis=0)\n",
    "                v = np.concatenate(\n",
    "                    (v, vtx, np.atleast_2d([np.nan, np.nan])), axis=0)\n",
    "            return v\n",
    "        \n",
    "        if len(dims) == 2:\n",
    "            pars['coordinates'] = get_slice_coords(Bmat)\n",
    "        else:\n",
    "            # make a list of the contour coordinates for each 2D slice\n",
    "            pars['coordinates'] = []\n",
    "            if slice_dim is None:\n",
    "                slice_dim = 0 if swap_dim else -1\n",
    "            for s in range(dims[slice_dim]):\n",
    "                B = Bmat.take(s, axis=slice_dim)\n",
    "                pars['coordinates'].append(get_slice_coords(B))\n",
    "\n",
    "        pars['neuron_id'] = i + 1\n",
    "        coordinates.append(pars)\n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca23c7c-a7ba-478c-a2c7-b1a156380c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, filename, pnr):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        # print(\"Keys: %s\" % f.keys())\n",
    "        # print(f['dims'])\n",
    "        # print(f['dview'])\n",
    "        # print(f['estimates'])\n",
    "        # print(f['mmap_file'])\n",
    "        # print(f['params'])\n",
    "        # print(f['remove_very_bad_comps'])\n",
    "        # print(f['skip_refinement'])\n",
    "    \n",
    "        data = f['estimates']['S_dff'][:]\n",
    "        # print(f['estimates'].keys())\n",
    "        # print(f['estimates']['A'])\n",
    "        A = f['estimates']['A']\n",
    "        dims = f['dims']\n",
    "        print(data.shape)\n",
    "        coordinates = get_contours(A,dims)\n",
    "    \n",
    "    \n",
    "    valid_index = []\n",
    "    centers = []\n",
    "    for i,c in enumerate(coordinates):\n",
    "        center = c['coordinates'][~(np.isnan(c['coordinates'][:,0])|np.isnan(c['coordinates'][:,1]))].mean(axis=0)\n",
    "        if np.isnan(center[0])==False and np.isnan(center[1])==False:\n",
    "            centers.append(center)\n",
    "            valid_index.append(i)\n",
    "            \n",
    "    centers = np.array(centers)\n",
    "    valid_index = np.array(valid_index)\n",
    "    \n",
    "    with open(pnr, 'rb') as file:\n",
    "        img = pickle.load(file)\n",
    "        plt.figure(figsize=(12.8, 8), dpi=150)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12.8, 8), dpi=150)\n",
    "    plt.xlim(0,img.shape[1])\n",
    "    plt.ylim(0,img.shape[0])\n",
    "    plt.scatter(centers[:,0],centers[:,1],s=10)\n",
    "    plt.gca().invert_yaxis()\n",
    "    for i,c in enumerate(centers):\n",
    "        plt.annotate(i,c,size=5)\n",
    "    plt.show()\n",
    "\n",
    "    return centers, img,valid_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f001d11-5baf-4f55-803f-7f780ac920e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'G:/mPFC-2/2025-05-18/'\n",
    "date = path[-11:-1]\n",
    "filename = path+date+\"_cnmf.hdf5\"\n",
    "pnr = path+date+\"_pnr.pickle\"\n",
    "\n",
    "centers_1,img_1,valid_index_1 = load_data(path,filename,pnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff67601-bd88-4346-9f34-9679e4746cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'G:/mPFC-3/2025-07-05/'\n",
    "filename = path+\"2025-07-05_cnmf.hdf5\"\n",
    "pnr = path+\"2025-07-05_pnr.pickle\"\n",
    "\n",
    "centers_2,img_2,valid_index_2 = load_data(path,filename,pnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4d3b5-fa23-42c2-9bb5-1271ff85e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'G:/mPFC-3/2025-07-11/'\n",
    "filename = path+\"2025-07-11_cnmf.hdf5\"\n",
    "pnr = path+\"2025-07-11_pnr.pickle\"\n",
    "\n",
    "centers_1,img_1,valid_index_1 = load_data(path,filename,pnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7b37a-42b7-49e0-ae2c-aa7c7aa71e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163eb847-27d1-40e6-ab54-03bbde8eab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(centers, src_points, dst_points, img):\n",
    "    # Compute the perspective transform matrix\n",
    "    matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "    \n",
    "    # Apply the perspective transformation\n",
    "    rectified_image = cv2.warpPerspective(img, matrix, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    plt.figure(figsize=(12.8, 8), dpi=150)\n",
    "    plt.imshow(rectified_image)\n",
    "    plt.show()\n",
    "    \n",
    "    # print(matrix)\n",
    "    # print(centers)\n",
    "    new_centers = []\n",
    "    for c in centers:\n",
    "        xy1 = np.array([c[0],c[1],1]).T\n",
    "        x_y_s = np.matmul(matrix,xy1)\n",
    "        # print(x_y_s)\n",
    "        new_centers.append([x_y_s[0]/x_y_s[2],x_y_s[1]/x_y_s[2]])\n",
    "    new_centers = np.array(new_centers)\n",
    "    plt.figure(figsize=(12.8, 8), dpi=150)\n",
    "    plt.xlim(0,img.shape[1])\n",
    "    plt.ylim(0,img.shape[0])\n",
    "    plt.scatter(new_centers[:,0],new_centers[:,1],s=10)\n",
    "    plt.gca().invert_yaxis()\n",
    "    for i,c in enumerate(new_centers):\n",
    "        plt.annotate(i,c,size=5)\n",
    "    plt.show()\n",
    "\n",
    "    return new_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8ebde-57bd-473f-9794-345fd9d11c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source points (corners of the distorted image)\n",
    "a = 10\n",
    "b = 203\n",
    "c = 602\n",
    "d = 740\n",
    "\n",
    "src_points = np.float32([[centers_1[a,0],centers_1[a,1]], [centers_1[b,0],centers_1[b,1]], [centers_1[c,0],centers_1[c,1]],[centers_1[d,0],centers_1[d,1]]])\n",
    "\n",
    "e = 6\n",
    "f = 196\n",
    "g = 569\n",
    "h = 740\n",
    "\n",
    "# Define the destination points (where you want the corners to be)\n",
    "dst_points = np.float32([[centers_2[e,0],centers_2[e,1]], [centers_2[f,0],centers_2[f,1]],[centers_2[g,0],centers_2[g,1]],[centers_2[h,0],centers_2[h,1]]])\n",
    "\n",
    "new_centers_1 = align(centers_1, src_points, dst_points, img_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28735bcf-433e-4f54-9148-81de3b7bfdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(array_1, array_2,img,threshold=20):\n",
    "    cost_values = []\n",
    "    pairs = []\n",
    "    cost_matrix = np.zeros([len(array_1),len(array_2)])\n",
    "    for i in range(len(array_1)):\n",
    "        for j in range(len(array_2)):\n",
    "            distance = np.linalg.norm(array_2[j,:] - array_1[i,:])\n",
    "            cost_matrix[i,j] = distance\n",
    "            cost_values.append(cost_matrix[i,j])\n",
    "            pairs.append([i,j])\n",
    "    cost_values = np.array(cost_values)\n",
    "    pairs = np.array(pairs)\n",
    "    sorted_index = np.argsort(cost_values)\n",
    "    cost_values = cost_values[sorted_index]\n",
    "    pairs = pairs[sorted_index]\n",
    "    registered_1 = []\n",
    "    registered_2 = []\n",
    "\n",
    "    corresponding_12 = {}\n",
    "    corresponding_21 = {}\n",
    "    for i,d in enumerate(cost_values):\n",
    "        if d > threshold or len(registered_1) >= min(len(array_1),len(array_2)):\n",
    "            break\n",
    "        if pairs[i][0] not in registered_1 and pairs[i][1] not in registered_2:\n",
    "            registered_1.append(pairs[i][0])\n",
    "            registered_2.append(pairs[i][1])\n",
    "            corresponding_12[pairs[i][0]] = pairs[i][1]\n",
    "            corresponding_21[pairs[i][1]] = pairs[i][0]\n",
    "            \n",
    "\n",
    "    plt.figure(figsize=(12.8, 8), dpi=150)\n",
    "    plt.scatter(array_1[registered_1,0],array_1[registered_1,1],s=10)\n",
    "    plt.scatter(array_2[registered_2,0],array_2[registered_2,1],s=10)\n",
    "    \n",
    "    \n",
    "    plt.xlim(0,img.shape[1])\n",
    "    plt.ylim(0,img.shape[0])\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.show()\n",
    "    return registered_1, registered_2, corresponding_12, corresponding_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342aebe-a5c0-46be-be71-953cac5dbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_1, registered_2, corresponding_12, corresponding_21 = register(new_centers_1, centers_2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265061e-9caf-445d-8b72-31aeffccb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(registered_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2fb37-aae0-414d-89cb-bd94a917644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuous_data(data, event, trial, padding,delay,interval,percentile,gaussian):\n",
    "    data = data.T\n",
    "\n",
    "\n",
    "    if interval > 0:\n",
    "        for i in range(int(data.shape[0]/interval)+1):\n",
    "            baseline = np.mean(data[i*interval:(i+1)*interval,:])\n",
    "            data[i*interval:(i+1)*interval,:] = data[i*interval:(i+1)*interval,:]/baseline\n",
    "\n",
    "    for n in range(data.shape[1]):\n",
    "        threshold = np.percentile(data[:,n][data[:,n]>0],percentile)\n",
    "        data[:,n][data[:,n]<threshold] = 0\n",
    "        \n",
    "    data = scipy.ndimage.gaussian_filter1d(data,2,axis=0)\n",
    "\n",
    "    data = data.T\n",
    "    \n",
    "    data = scipy.stats.zscore(data, axis=1)\n",
    "    data = data[:, round(event[trial[0]])-padding[0]:round(event[trial[1]])+padding[1]] \n",
    "    \n",
    "    event_index = []\n",
    "    for i in range(trial[0], trial[1]+1):\n",
    "        event_index.append(round(event[i])-(round(event[trial[0]])-padding[0]))\n",
    "    \n",
    "    data = np.array(data)\n",
    "    event_index = np.array(event_index)\n",
    "\n",
    "    # Substract delay\n",
    "    delay = delay\n",
    "    event_index_new = []\n",
    "    for i,e in enumerate(event_index):\n",
    "        event_index_new.append(round(e+i*delay))\n",
    "    event_index = event_index_new\n",
    "\n",
    "    return data, event_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3427346-3023-41b9-93a1-371a43c805ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'G:/mPFC-2/2025-05-15/'\n",
    "filename = path+\"2025-05-15_cnmf.hdf5\"\n",
    "Trial_idx_file = path+'Trial_idx1.mat'\n",
    "Trial_idx = scipy.io.loadmat(Trial_idx_file)['Trial_idx1']\n",
    "door_open = Trial_idx[:,1]\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    data = f['estimates']['S_dff'][:]\n",
    "print(data.shape)\n",
    "data, door_open_index = generate_continuous_data(data[valid_index_1], door_open, [0,len(door_open)-1], [100,200],0.13,1000,30,0.1)\n",
    "\n",
    "single_trial_data = []\n",
    "for i,e in enumerate(door_open_index):\n",
    "    single_trial_data.append(data[registered_1,e-50:e+50])\n",
    "trial_averaged_data = np.mean(single_trial_data,axis=0)\n",
    "sorted_index = np.argsort(np.argmax(trial_averaged_data,axis=1))\n",
    "seaborn.heatmap(trial_averaged_data[sorted_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd0156-288a-4627-82b5-03c407467f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'G:/mPFC-3/2025-07-12/'\n",
    "filename = path+\"2025-07-12_cnmf.hdf5\"\n",
    "Trial_idx_file = path+'Trial_idx1.mat'\n",
    "Trial_idx = scipy.io.loadmat(Trial_idx_file)['Trial_idx1']\n",
    "door_open = Trial_idx[:,1]\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    data = f['estimates']['S_dff'][:]\n",
    "print(data.shape)\n",
    "data, door_open_index = generate_continuous_data(data[valid_index_2], door_open, [0,len(door_open)-1], [100,200],0.13,1000,30,0.1)\n",
    "\n",
    "single_trial_data = []\n",
    "for i,e in enumerate(door_open_index):\n",
    "    single_trial_data.append(data[registered_2,e-50:e+50])\n",
    "trial_averaged_data = np.mean(single_trial_data,axis=0)\n",
    "seaborn.heatmap(trial_averaged_data[sorted_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3d5ad-6b93-42b9-ad6e-a6e937b52b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa35b2d-13bd-48be-90e2-e4059102b943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ca66f-592f-4927-a10b-d25bbec3ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi alignment\n",
    "# sessions\n",
    "Sessions = ['G:/mPFC-3/2025-07-04/','G:/mPFC-3/2025-07-05/','G:/mPFC-3/2025-07-06/','G:/mPFC-3/2025-07-08/',\n",
    "            'G:/mPFC-3/2025-07-11/','G:/mPFC-3/2025-07-13/']\n",
    "\n",
    "# template\n",
    "template_path = 'G:/mPFC-3/2025-07-12/'\n",
    "\n",
    "# Align points\n",
    "Points = [[32,167,371,329,18,196,589,696],\n",
    "          [19,74,362,399,18,489,589,648],\n",
    "          [24,89,234,287,28,489,594,646],\n",
    "          [35,219,554,665,65,213,588,720],\n",
    "          [10,203,602,740,6,196,569,740],\n",
    "          [48,238,517,636,18,197,751,805]]\n",
    "\n",
    "# # sessions\n",
    "# Sessions = ['G:/mPFC-1/2025-05-08/','G:/mPFC-1/2025-05-10/','G:/mPFC-1/2025-05-18/']\n",
    "\n",
    "# # template\n",
    "# template_path = 'G:/mPFC-1/2025-05-15/'\n",
    "\n",
    "# # Align points\n",
    "# Points = [[7,64,75,186,3,155,213,380],\n",
    "#           [1,93,359,459,6,173,236,297],\n",
    "#          [2,35,167,238,3,48,221,284]]\n",
    "\n",
    "# # sessions\n",
    "# Sessions = ['G:/mPFC-2/2025-05-08/','G:/mPFC-2/2025-05-18/']\n",
    "\n",
    "# # template\n",
    "# template_path = 'G:/mPFC-2/2025-05-15/'\n",
    "\n",
    "# # Align points\n",
    "# Points = [[132,232,365,540,145,271,403,575],\n",
    "#          [21,170,253,403,2,278,385,559]]\n",
    "\n",
    "# # sessions\n",
    "# Sessions = ['G:/M2-1/2025-03-26/','G:/M2-1/2025-04-05/']\n",
    "\n",
    "# # template\n",
    "# template_path = 'G:/M2-1/2025-04-01/'\n",
    "\n",
    "# # Align points\n",
    "# Points = [[37,132,242,204,22,176,375,439],\n",
    "#           [12,170,251,361,24,177,280,284]]\n",
    "\n",
    "# Load template\n",
    "date = template_path[-11:-1]\n",
    "filename = template_path+date+\"_cnmf.hdf5\"\n",
    "pnr = template_path+date+\"_pnr.pickle\"\n",
    "centers_2,img_2,valid_index_2 = load_data(template_path,filename,pnr)\n",
    "\n",
    "valid_indices = []\n",
    "R2 = []\n",
    "C21 = []\n",
    "for path, points in zip(Sessions,Points):\n",
    "    date = path[-11:-1]\n",
    "    filename = path+date+\"_cnmf.hdf5\"\n",
    "    pnr = path+date+\"_pnr.pickle\"\n",
    "    centers_1,img_1,valid_index_1 = load_data(path,filename,pnr)\n",
    "    valid_indices.append(valid_index_1)\n",
    "    \n",
    "    a = points[0]\n",
    "    b = points[1]\n",
    "    c = points[2]\n",
    "    d = points[3]\n",
    "    e = points[4]\n",
    "    f = points[5]\n",
    "    g = points[6]\n",
    "    h = points[7]\n",
    "    src_points = np.float32([[centers_1[a,0],centers_1[a,1]], \n",
    "                             [centers_1[b,0],centers_1[b,1]], \n",
    "                             [centers_1[c,0],centers_1[c,1]],\n",
    "                             [centers_1[d,0],centers_1[d,1]]])\n",
    "    dst_points = np.float32([[centers_2[e,0],centers_2[e,1]], \n",
    "                             [centers_2[f,0],centers_2[f,1]],\n",
    "                             [centers_2[g,0],centers_2[g,1]],\n",
    "                             [centers_2[h,0],centers_2[h,1]]])\n",
    "    \n",
    "    new_centers_1 = align(centers_1, src_points, dst_points, img_1)\n",
    "    registered_1, registered_2, corresponding_12, corresponding_21 = register(new_centers_1, centers_2,img_1,50)\n",
    "    R2.append(registered_2)\n",
    "    C21.append(corresponding_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53af166-eb1c-4412-ad27-c37738b96713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align all sessions\n",
    "registered_neurons = np.arange(len(valid_index_2))\n",
    "for r in R2:\n",
    "    registered_neurons = np.intersect1d(registered_neurons,r)\n",
    "print(len(registered_neurons))\n",
    "Reg_Neu = []\n",
    "for c in C21:\n",
    "    reg_neu = []\n",
    "    for n in registered_neurons:\n",
    "        reg_neu.append(c[n])\n",
    "    Reg_Neu.append(reg_neu)\n",
    "Reg_Neu = np.array(Reg_Neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41571bb-7031-4059-84a9-3db21e1549c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabba3b-7a09-44a1-8fa5-324459fdb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rastermap sorting\n",
    "date = template_path[-11:-1]\n",
    "filename = template_path+date+\"_cnmf.hdf5\"\n",
    "Trial_idx_file = template_path+'Trial_idx1.mat'\n",
    "Trial_idx = scipy.io.loadmat(Trial_idx_file)['Trial_idx1']\n",
    "door_open = Trial_idx[:,1]\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    data = f['estimates']['S_dff'][:]\n",
    "print(data.shape)\n",
    "template_data, template_door_open_index = generate_continuous_data(data[valid_index_2], door_open, [0,len(door_open)-1], [100,200],0.13,1000,30,0.1)\n",
    "\n",
    "single_trial_data = []\n",
    "for i,e in enumerate(template_door_open_index):\n",
    "    single_trial_data.append(template_data[registered_neurons,e-50:e+50])\n",
    "trial_averaged_data = np.mean(single_trial_data,axis=0)\n",
    "sorted_index = np.argsort(np.argmax(trial_averaged_data,axis=1))\n",
    "\n",
    "model = Rastermap(locality=0.2, time_lag_window=20).fit(template_data[registered_neurons])\n",
    "sorted_order = model.isort\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data = []\n",
    "Event_Index = []\n",
    "# Average\n",
    "for idx, path in enumerate(Sessions):\n",
    "    print(path)\n",
    "    date = path[-11:-1]\n",
    "    filename = path+date+\"_cnmf.hdf5\"\n",
    "    Trial_idx_file = path+'Trial_idx1.mat'\n",
    "    Trial_idx = scipy.io.loadmat(Trial_idx_file)['Trial_idx1']\n",
    "    door_open = Trial_idx[:,1]\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        data = f['estimates']['S_dff'][:]\n",
    "    print(data.shape)\n",
    "    data, door_open_index = generate_continuous_data(data[valid_indices[idx]], door_open, [0,len(door_open)-1], [100,200],0.13,1000,30,0.1)\n",
    "    Data.append(data[Reg_Neu[idx]])\n",
    "    Event_Index.append(door_open_index)\n",
    "    single_trial_data = []\n",
    "    for i,e in enumerate(door_open_index):\n",
    "        single_trial_data.append(data[Reg_Neu[idx],e-50:e+50])\n",
    "    trial_averaged_data = np.mean(single_trial_data,axis=0)\n",
    "    # if idx == 0:\n",
    "    #     sorted_index = np.argsort(np.argmax(trial_averaged_data,axis=1))\n",
    "        \n",
    "        # # Rastermap sorting\n",
    "        # model = Rastermap(locality=0.2, time_lag_window=20).fit(data[Reg_Neu[idx]])\n",
    "        # sorted_order = model.isort\n",
    "    seaborn.heatmap(trial_averaged_data[sorted_index])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(150,2))\n",
    "    seaborn.heatmap(data[Reg_Neu[idx]][sorted_order],vmin=0,vmax=5,cmap=\"gray_r\")\n",
    "    for i,e in enumerate(door_open_index):\n",
    "        plt.plot([e-20,e-20],[0,data.shape[0]], color='blue',linestyle='--', linewidth=0.4)\n",
    "        plt.plot([e+50,e+50],[0,data.shape[0]], color='grey',linestyle='--', linewidth=0.4)\n",
    "    plt.show()\n",
    "\n",
    "# Insert template\n",
    "# Data.insert(1,template_data[registered_neurons])\n",
    "# Event_Index.insert(1,template_door_open_index)\n",
    "Data.insert(5,template_data[registered_neurons])\n",
    "Event_Index.insert(5,template_door_open_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407d044-8a72-4af2-83d3-e1fc9e1cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_session_cat_data = np.concatenate(Data,axis=1)\n",
    "print(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30e002-a85b-4fa3-acb1-6530bffa149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(transform_data, transform_event_index, fit_data, fit_event_index, n_components=3,mask_period=[-20,50]):\n",
    "\n",
    "    mask = np.ones([fit_data.shape[1]],dtype=bool)\n",
    "    for i in fit_event_index:\n",
    "        mask[i+mask_period[0]:i+mask_period[1]] = False\n",
    "        \n",
    "    reducer = decomposition.PCA(n_components)\n",
    "    # Transpose\n",
    "    fit_data = fit_data.T\n",
    "    transform_data = transform_data.T\n",
    "    \n",
    "    # Fit model\n",
    "    reducer.fit(fit_data[mask])\n",
    "\n",
    "    # Transform\n",
    "    embeddings = reducer.transform(transform_data)\n",
    "    print(reducer.explained_variance_ratio_)\n",
    "\n",
    "    for i in range(n_components):\n",
    "        plt.figure(figsize=(150, 5), dpi=80)\n",
    "        for j in transform_event_index:\n",
    "            plt.plot([j/10-2,j/10-2], [-5, 5], color='black', linestyle='--')\n",
    "            plt.plot([j/10+5,j/10+5], [-5, 5], color='grey', linestyle='--')\n",
    "        plt.plot(np.array(range(len(embeddings[:,i])))/10,embeddings[:,i])\n",
    "        plt.show()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9328e3-4933-4c22-a6ab-6959d9fd36aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = 5\n",
    "fit = 5\n",
    "embeddings = PCA(Data[transform],Event_Index[transform], Data[fit],Event_Index[fit],10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ebddc-2456-406a-badc-3a895f7d08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(150, 5), dpi=80)\n",
    "\n",
    "# Load behavior file\n",
    "path = 'G:/mPFC-3/2025-07-12/'\n",
    "behav_file = path+'behav_score.xlsx'\n",
    "bf = pd.read_excel(behav_file, header=None)\n",
    "label = np.squeeze(bf.values)\n",
    "print(label)\n",
    "plt.plot(embeddings[:,5])\n",
    "\n",
    "for i,e in enumerate(Event_Index[transform]):\n",
    "    if label[i] == \"ss\" or label[i] ==\"ms\":\n",
    "        plt.plot([e-20,e-20],[-5,5], color='green',linestyle='--')\n",
    "        plt.plot([e+50,e+50],[-5,5], color='grey',linestyle='--')\n",
    "    elif label[i] == \"sf\" or label[i] ==\"mf\":\n",
    "        plt.plot([e-20,e-20],[-5,5], color='red',linestyle='--')\n",
    "        plt.plot([e+50,e+50],[-5,5], color='grey',linestyle='--')\n",
    "    elif label[i] == \"n\":\n",
    "        plt.plot([e-20,e-20],[-5,5], color='black',linestyle='--')\n",
    "        plt.plot([e+50,e+50],[-5,5], color='grey',linestyle='--')\n",
    "    else:\n",
    "        plt.plot([e-20,e-20],[-5,5], color='blue',linestyle='--')\n",
    "        plt.plot([e+50,e+50],[-5,5], color='grey',linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a137d-78b6-4065-b8ff-53f23278d64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15150a-fde2-4fa8-9021-41cc4fc5fe30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
